12.4 Neural Networks
Neural networks provide a flexible non-linear extension of multiple logistic regression, as we saw in Section 8.10. We can consider them for the Cushing's syndrome example by the following code. ${ }^2$

The results are shown in Figure 12.7. We see that in all cases there are multiple local maxima of the likelihood, since different runs gave different classifiers.
Once we have a penalty, the choice of the number of hidden units is often not critical (see Figure 12.7). The spirit of the predictive approach is to average the predicted $p(c \mid \boldsymbol{x})$ over the local maxima. A simple average will often suffice: m

Note that there are two quite different types of local maxima occurring here, and some local maxima occur several times (up to convergence tolerances). An average does better than either type of classifier.
12.5 Support Vector Machines
Support vector machines (SVMs) are the latest set of methods within this field. They have been promoted enthusiastically, but with little respect to the selection effects of choosing the test problem and the member of the large class of classifiers to present. The original ideas are in Boser et al. (1992); Cortes and Vapnik (1995); Vapnik (1995, 1998); the books by Cristianini and Shawe-Taylor (2000) and Hastie et al. (2001, $\ 4.5,12.2,12.3)$ present the underlying theory.

The method for $g=2$ classes is fairly simple to describe. Logistic regression will fit exactly in separable cases where there is a hyperplane that has all class-one points on one side and all class-two points on the other. It would be a coincidence for there to be only one such hyperplane, and fitting a logistic regression will tend to fit a decision surface $p(2 \mid \boldsymbol{x})=0.5$ in the middle of the 'gap' between the groups. Support vector methods attempt directly to find a hyperplane in the middle of the gap, that is with maximal margin (the distance from the hyperplane to the nearest point). This is quadratic programming problem that can be solved by standard methods. ${ }^3$ Such a hyperplane has support vectors, data points that are exactly the margin distance away from the hyperplane. It will typically be a very good classifier.

The problem is that usually no separating hyperplane will exist. This difficulty is tackled in two ways. First, we can allow some points to be on the wrong side of their margin (and for some on the wrong side of the hyperplane) subject to a constraint on the total of the 'mis-fit' distances being less than some constant, with Lagrange multiplier $C>0$. This is still a quadratic programming problem, because of the rather arbitrary use of sum of distances.

Second, the set of variables is expanded greatly by taking non-linear functions of the original set of variables. Thus rather than seeking a classifying hyperplane $f(x)=\boldsymbol{x}^T \boldsymbol{\beta}+\beta_0=0$, we seek $f(x)=h(\boldsymbol{x})^T \boldsymbol{\beta}+\beta_0=0$ for a vector of $M \gg p$ functions $h_i$. Then finding a optimal separating hyperplane is equivalent to solving
$$
\min _{\beta_0, \boldsymbol{\beta}} \sum_{i=1}^n\left[1-y_i f\left(\boldsymbol{x}_i\right)\right]_{+}+\frac{1}{2 C}\|\boldsymbol{\beta}\|^2
$$
where $y_i=\pm 1$ for the two classes. This is yet another penalized fitting problem, not dissimilar (Hastie et al., 2001, p. 380) to a logistic regression with weight decay (which can be fitted by multinom). The claimed advantage of SVMs is that because we only have to find the support vectors, the family of functions $h$ can be large, even infinite-dimensional.

There is an implementation of SVMs for R in function svm in package e1071. ${ }^4$ The default values do not do well, but after some tuning for the crabs data we can get a good discriminant with 21 support vectors. Here cost is $C$ and gamma is a coefficient of the kernel used to form $h$.

We can try a 10-fold cross-validation by m

The extension to $g>2$ classes is much less elegant, and several ideas have been used. The svm function uses one attributed to Knerr $e$ al. (1990) in which classifiers are built comparing each pair of classes, and the majority vote amongst the resulting $g(g-1) / 2$ classifiers determines the predicted class.
12.6 Forensic Glass Example
The forensic glass dataset $\mathrm{fgl}$ has 214 points from six classes with nine measurements, and provides a fairly stiff test of classification methods. As we have seen (Figures $4.17$ on page $99,5.4$ on page $116,11.5$ on page 309 and $12.3$ on page 337) the types of glass do not form compact well-separated groupings, and the marginal distributions are far from normal. There are some small classes (with 9,13 and 17 examples), so we cannot use quadratic discriminant analysis.

We assess their performance by 10 -fold cross-validation, using the same random partition for all the methods. Logistic regression provides a suitable benchmark (as is often the case), and in this example linear discriminant analysis does equally well.

We can use nearest-neighbour methods to estimate the lower bound on the Bayes risk as about 10% (Ripley, 1996, pp. 196-7).

We saw in Chapter 9 that we could fit a classification tree of size about six to this dataset. We need to cross-validate over the choice of tree size, which does vary by group from four to seven.

We wrote some general functions for testing neural network models by $V$-fold cross-validation. First we rescale the dataset so the inputs have range $[0,1]$.

It is straightforward to fit a fully specified neural network. However, we want to average across several fits and to choose the number of hidden units and the amount of weight decay by an inner cross-validation. To do so we wrote a fairly general function that can easily be used or modified to suit other problems. (See the scripts for the code.)

This fits a neural network 1000 times, and so is fairly slow (about half an hour on the PC).

This code chooses between neural nets on the basis of their cross-validated error rate. An alternative is to use logarithmic scoring, which is equivalent to finding the deviance on the validation set. Rather than count 0 if the predicted class is correct and 1 otherwise, we count $-\log p(c \mid x)$ for the true class $c$. We can easily code this variant by replacing the line m by n in CVnn2.

Support Vector machines

The foHowing is faster, but not strict1y comparable with the results above, as a different random partition will be used.

Learning vector quantization
For LVQ as for k-nearest neighbour methods we have to select a suitable metric. The following experiments used Euclidean distance on the original variables, but the rescaled variables or Mahalanobis distance could also be tried.

We set an even prior over the c1asses as otherwise there are too few representatives of the smaller c1asses. Our initialization code in 1vqinit follows Kohonen's in selecting the number of representatives; in this problem 24 points are selected, four from each c1ass.

The initialization is random, so your results are likely to differ.


